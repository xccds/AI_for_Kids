{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 用全连接神经网络判断数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_size = 784\n",
    "hidden_size = 100\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载MNIST数据 \n",
    "train_dataset = torchvision.datasets.MNIST(root='../data/MNIST_data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../data/MNIST_data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i,(images,labels) in  enumerate(train_loader):\n",
    "    plt.imshow(images[0,0,:,:],cmap='gray');\n",
    "    if i > 1:\n",
    "        print(images.shape)\n",
    "        print(labels.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.4234\n",
      "Epoch [1/5], Step [200/600], Loss: 0.3023\n",
      "Epoch [1/5], Step [300/600], Loss: 0.2520\n",
      "Epoch [1/5], Step [400/600], Loss: 0.3481\n",
      "Epoch [1/5], Step [500/600], Loss: 0.3420\n",
      "Epoch [1/5], Step [600/600], Loss: 0.1613\n",
      "Epoch [2/5], Step [100/600], Loss: 0.4105\n",
      "Epoch [2/5], Step [200/600], Loss: 0.2096\n",
      "Epoch [2/5], Step [300/600], Loss: 0.1796\n",
      "Epoch [2/5], Step [400/600], Loss: 0.2246\n",
      "Epoch [2/5], Step [500/600], Loss: 0.1260\n",
      "Epoch [2/5], Step [600/600], Loss: 0.0974\n",
      "Epoch [3/5], Step [100/600], Loss: 0.1663\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0961\n",
      "Epoch [3/5], Step [300/600], Loss: 0.1397\n",
      "Epoch [3/5], Step [400/600], Loss: 0.1200\n",
      "Epoch [3/5], Step [500/600], Loss: 0.2421\n",
      "Epoch [3/5], Step [600/600], Loss: 0.1765\n",
      "Epoch [4/5], Step [100/600], Loss: 0.1747\n",
      "Epoch [4/5], Step [200/600], Loss: 0.1071\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0898\n",
      "Epoch [4/5], Step [400/600], Loss: 0.1309\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0748\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0971\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0772\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0638\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0940\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0462\n",
      "Epoch [5/5], Step [500/600], Loss: 0.1702\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0300\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        images = images.reshape(-1, 28*28)      \n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97.19 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  用全连接神经网络判断服饰类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='../data/fasion_mnist', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='../data/fasion_mnist', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i,(images,labels) in  enumerate(train_loader):\n",
    "    plt.imshow(images[0,0,:,:],cmap='gray');\n",
    "    if i > 1:\n",
    "        print(images.shape)\n",
    "        print(labels.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fashion = NeuralNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_fashion.parameters(), lr=learning_rate)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 11.9489\n",
      "Epoch [1/5], Step [200/600], Loss: 13.1135\n",
      "Epoch [1/5], Step [300/600], Loss: 13.5063\n",
      "Epoch [1/5], Step [400/600], Loss: 12.8764\n",
      "Epoch [1/5], Step [500/600], Loss: 14.5751\n",
      "Epoch [1/5], Step [600/600], Loss: 12.2308\n",
      "Epoch [2/5], Step [100/600], Loss: 10.4673\n",
      "Epoch [2/5], Step [200/600], Loss: 12.6565\n",
      "Epoch [2/5], Step [300/600], Loss: 12.4745\n",
      "Epoch [2/5], Step [400/600], Loss: 10.6528\n",
      "Epoch [2/5], Step [500/600], Loss: 9.5800\n",
      "Epoch [2/5], Step [600/600], Loss: 11.9939\n",
      "Epoch [3/5], Step [100/600], Loss: 12.0565\n",
      "Epoch [3/5], Step [200/600], Loss: 12.3040\n",
      "Epoch [3/5], Step [300/600], Loss: 10.8829\n",
      "Epoch [3/5], Step [400/600], Loss: 12.0260\n",
      "Epoch [3/5], Step [500/600], Loss: 11.3427\n",
      "Epoch [3/5], Step [600/600], Loss: 14.3680\n",
      "Epoch [4/5], Step [100/600], Loss: 11.5003\n",
      "Epoch [4/5], Step [200/600], Loss: 11.8644\n",
      "Epoch [4/5], Step [300/600], Loss: 13.1873\n",
      "Epoch [4/5], Step [400/600], Loss: 13.0958\n",
      "Epoch [4/5], Step [500/600], Loss: 11.6420\n",
      "Epoch [4/5], Step [600/600], Loss: 12.6640\n",
      "Epoch [5/5], Step [100/600], Loss: 12.1797\n",
      "Epoch [5/5], Step [200/600], Loss: 10.4746\n",
      "Epoch [5/5], Step [300/600], Loss: 11.7386\n",
      "Epoch [5/5], Step [400/600], Loss: 12.4845\n",
      "Epoch [5/5], Step [500/600], Loss: 11.8010\n",
      "Epoch [5/5], Step [600/600], Loss: 13.9974\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        images = images.reshape(-1, 28*28)      \n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 7.65 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 用卷积神经网络判断服饰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 5\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cnn_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(14*14*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.cnn_layer(x)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fashion_cnn = ConvNet(num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_fashion_cnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.4598\n",
      "Epoch [1/5], Step [200/600], Loss: 0.5903\n",
      "Epoch [1/5], Step [300/600], Loss: 0.3828\n",
      "Epoch [1/5], Step [400/600], Loss: 0.4964\n",
      "Epoch [1/5], Step [500/600], Loss: 0.3470\n",
      "Epoch [1/5], Step [600/600], Loss: 0.3416\n",
      "Epoch [2/5], Step [100/600], Loss: 0.3006\n",
      "Epoch [2/5], Step [200/600], Loss: 0.3538\n",
      "Epoch [2/5], Step [300/600], Loss: 0.5296\n",
      "Epoch [2/5], Step [400/600], Loss: 0.2037\n",
      "Epoch [2/5], Step [500/600], Loss: 0.1881\n",
      "Epoch [2/5], Step [600/600], Loss: 0.2037\n",
      "Epoch [3/5], Step [100/600], Loss: 0.3125\n",
      "Epoch [3/5], Step [200/600], Loss: 0.3285\n",
      "Epoch [3/5], Step [300/600], Loss: 0.2498\n",
      "Epoch [3/5], Step [400/600], Loss: 0.2975\n",
      "Epoch [3/5], Step [500/600], Loss: 0.3747\n",
      "Epoch [3/5], Step [600/600], Loss: 0.3266\n",
      "Epoch [4/5], Step [100/600], Loss: 0.3912\n",
      "Epoch [4/5], Step [200/600], Loss: 0.3139\n",
      "Epoch [4/5], Step [300/600], Loss: 0.2289\n",
      "Epoch [4/5], Step [400/600], Loss: 0.2697\n",
      "Epoch [4/5], Step [500/600], Loss: 0.2292\n",
      "Epoch [4/5], Step [600/600], Loss: 0.2842\n",
      "Epoch [5/5], Step [100/600], Loss: 0.3449\n",
      "Epoch [5/5], Step [200/600], Loss: 0.2159\n",
      "Epoch [5/5], Step [300/600], Loss: 0.2130\n",
      "Epoch [5/5], Step [400/600], Loss: 0.2445\n",
      "Epoch [5/5], Step [500/600], Loss: 0.1951\n",
      "Epoch [5/5], Step [600/600], Loss: 0.3480\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        outputs = model_fashion_cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 90.06 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model_fashion_cnn(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
