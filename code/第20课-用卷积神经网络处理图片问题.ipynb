{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本课提纲\n",
    "- 用全连接神经网络判断数字\n",
    "- 用全连接神经网络判断服饰\n",
    "- 卷积神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 用全连接神经网络判断数字"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在前面的课程中，我们用全连接神经网络建立一个预测模型来预测花的种类，在iris数据中，给定的四个特征都是人工测量计算好的，神经网络将其作为输入特征来汇总计算的。其实神经网络还可以处理更为复杂的工作，例如说，只给出一朵花的图片，能不能直接预测出花的种类呢？这个任务的难点在于，没有告诉计算机特征是什么，需要计算机自己去计算所需要的特征。我们先来看一个简单点的例子，就是给定一个图片，来判断这个图片中的数字是多少。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先仍是加载必要的模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型的超参数，这里我们输入的图片大小是28\\*28大小的，所以一共包括了784个像素点，也就是784个输入，把这个原始的像素值直接当输入特征。然后我们要输出预测的是从0到9的一共10个数字，所以是一个多分类的问题，具体的讲是10分类。设置的隐层神经元数量是100，循环次数是5次，每个批次的数据是100个，学习率0.001。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 784\n",
    "hidden_size = 100\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读入数据阶段，第一步是构造一个dataset，定义了它的位置和处理步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='../data/MNIST_data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../data/MNIST_data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后再用DataLoader定义数据如何读进来，批次大小，是否进行shuffle等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们读入一张图片来试试看。注意train_loader还只是一个迭代器，需要通过一个循环来把数据真正的读进来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADjJJREFUeJzt3X+MVfWZx/HPg8AfQgVNI4sCSyUobojYzcRsAjGa1epuUOCPaiUajE2nJmBoYuIaY4LJ2qRZt6z+hdI4liZAaaIuhBRLg+tSjDGMsFQpS2sqW1gIowEt/UNR5tk/5rAZcO733Ln3/Bqf9yshc+997jnnyR0+c86933PP19xdAOIZV3cDAOpB+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBDW+yo2ZGacTAiVzd2vneV3t+c3sTjM7bGbvm9nj3awLQLWs03P7zewSSb+XdLukY5L2SrrP3X+XWIY9P1CyKvb8N0l6393/6O5nJf1c0pIu1gegQt2E/2pJR4fdP5Y9dgEz6zWzfjPr72JbAArWzQd+Ix1afOmw3t3XS1ovcdgPNEk3e/5jkmYOuz9D0vHu2gFQlW7Cv1fSXDP7hplNlPQdSduKaQtA2To+7Hf3L8xslaRfSbpEUp+7HyysMwCl6nior6ON8Z4fKF0lJ/kAGLsIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiq0im68dUzYcKEZH3mzJnJesqCBQuS9QMHDiTrx4+3nkPm008/7ainrxL2/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QVFfj/GZ2RNIZSeckfeHuPUU0heLMmzcvWV+0aFGyvnjx4mR90qRJyfqtt96arJepr6+vZW3Tpk3JZd94442Cu2meIk7yudXdPypgPQAqxGE/EFS34XdJO83sHTPrLaIhANXo9rB/obsfN7MrJf3azP7b3XcPf0L2R4E/DEDDdLXnd/fj2c8BSa9KummE56x39x4+DASapePwm9kkM/va+duSviXpvaIaA1Cubg77p0l61czOr2eTu79WSFcASmfuXt3GzKrbWCDPPvtsy9r999+fXHbq1KldbTv7499Slf+/RmNwcDBZ37NnT7Ke97qmriVQNndP/1IyDPUBQRF+ICjCDwRF+IGgCD8QFOEHgmKorwHyLn+9du3aZP3hhx9uWcsbiuvWWB3q69ahQ4eS9ZtvvjlZP336dJHtXIChPgBJhB8IivADQRF+ICjCDwRF+IGgCD8QFOP8FZgyZUqyvmXLlmT9tttuK7KdQpU5zv/cc88l63v37k3Wr7322pa1hx56KLnsjBkzkvU8jzzySLK+bt26rtafwjg/gCTCDwRF+IGgCD8QFOEHgiL8QFCEHwiqiFl6kePpp59O1ps8jp/nhRdeSNY3bNjQ8br7+/uT9XPnznW87o0bNybrhw8f7njdkjR37tyulq8Ce34gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCCp3nN/M+iQtljTg7vOzx66QtEXSbElHJN3j7uVdiLzhHnzwwWQ9dV39sW716tXJ+ueff15RJ6Nz9OjRZP3AgQPJ+oIFC5L1vLkYmqCdPf9PJd150WOPS9rl7nMl7cruAxhDcsPv7rslnbro4SWSzp+6tUHS0oL7AlCyTt/zT3P3E5KU/byyuJYAVKH0c/vNrFdSb9nbATA6ne75T5rZdEnKfg60eqK7r3f3Hnfv6XBbAErQafi3SVqR3V4haWsx7QCoSm74zWyzpLckXWdmx8zsu5J+JOl2M/uDpNuz+wDGEK7b36Z58+a1rO3Zsye57NSpU4tupzBnzpxJ1rduTR/UPfPMM8n6wYMHR91TFcaNS+/38uZSWLZsWVfbHz++vI/buG4/gCTCDwRF+IGgCD8QFOEHgiL8QFBcurtNK1eubFkby0N5y5cvT9Z37NhRZDuNcemllybr3Q7ljQXs+YGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMb5M3fffXeynnd57jqlLo997733JpfduXNn0e1gjGDPDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBMc6fybtE9eDgYEWdjN6aNWta1hjHH9lll12WrJu1dfXrlnbv3t3V8lVgzw8ERfiBoAg/EBThB4Ii/EBQhB8IivADQeWO85tZn6TFkgbcfX722FOSvifpw+xpT7j7L8tqsgp54/hVTmU+Wi+99FLdLYw5Tz75ZLLe7e97//79XS1fhXb2/D+VdOcIj/+bu9+Y/RvTwQciyg2/u++WdKqCXgBUqJv3/KvM7Ldm1mdmlxfWEYBKdBr+dZLmSLpR0glJP271RDPrNbN+M+vvcFsAStBR+N39pLufc/dBST+RdFPiuevdvcfdezptEkDxOgq/mU0fdneZpPeKaQdAVdoZ6tss6RZJXzezY5LWSLrFzG6U5JKOSPp+iT0CKEFu+N39vhEefrGEXsasw4cPJ+tz5sxJ1sePT/8aduzYkayfPn06WY8q9bovX768wk6aiTP8gKAIPxAU4QeCIvxAUIQfCIrwA0Fx6e4CvP7668n6Nddc09X6P/vss2S9yZcVL9OUKVOS9UcffbRlbfLkyUW3c4G33nqr1PUXgT0/EBThB4Ii/EBQhB8IivADQRF+ICjCDwTFOH/mtddeS9bvuOOOlrUZM2Ykl+12HH7p0qXJ+mOPPday1tfXl1x2YGCgo56qsHr16q7qs2bNKrKdC+R9jfrNN98sbdtFYc8PBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0FZlVNPm1lj57m+4YYbkvXt27e3rF111VVFt1OYDz74IFn/8MMPk/U8Zpasd/P/q6cnPcnTuHH17bseeOCBZH3z5s0VdfJl7p7+pWTY8wNBEX4gKMIPBEX4gaAIPxAU4QeCIvxAULnj/GY2U9LPJP2VpEFJ6939OTO7QtIWSbMlHZF0j7snv+Tc5HH+PPPnz29Zy5tCe/r06UW30xhljvPXKe86CKtWrUrWz549W2Q7o1LkOP8Xkh519+sl/Z2klWb2N5Iel7TL3edK2pXdBzBG5Ibf3U+4+77s9hlJhyRdLWmJpA3Z0zZISl9uBkCjjOo9v5nNlvRNSW9LmubuJ6ShPxCSriy6OQDlafsafmY2WdLLkn7g7n/Oe683bLleSb2dtQegLG3t+c1sgoaCv9HdX8kePmlm07P6dEkjXgnS3de7e4+7p7+lAaBSueG3oV38i5IOufvaYaVtklZkt1dI2lp8ewDK0s5Q3yJJv5H0roaG+iTpCQ297/+FpFmS/iTp2+5+KmddY3PcJ8d1112XrG/ZsiVZTw0jNl2dQ31Hjx5N1j/55JOWteeffz65bF69ydod6st9z+/ueyS1Wtnfj6YpAM3BGX5AUIQfCIrwA0ERfiAowg8ERfiBoLh0dwUmTpyYrC9cuDBZzzuPYOXKlS1r119/fXLZbpU5zr9v375k/a677krWT5482fG2xzIu3Q0gifADQRF+ICjCDwRF+IGgCD8QFOEHgmKc/ytg6tSpHdUkadmyZcn6rl27kvU5c+Yk6/v370/WUz7++OOu6lExzg8gifADQRF+ICjCDwRF+IGgCD8QFOEHgmKcH/iKYZwfQBLhB4Ii/EBQhB8IivADQRF+ICjCDwSVG34zm2lm/2Fmh8zsoJmtzh5/ysz+18z+K/v3j+W3C6AouSf5mNl0SdPdfZ+ZfU3SO5KWSrpH0l/c/V/b3hgn+QCla/ckn/FtrOiEpBPZ7TNmdkjS1d21B6Buo3rPb2azJX1T0tvZQ6vM7Ldm1mdml7dYptfM+s2sv6tOARSq7XP7zWyypP+U9EN3f8XMpkn6SJJL+mcNvTV4KGcdHPYDJWv3sL+t8JvZBEnbJf3K3deOUJ8tabu7z89ZD+EHSlbYF3tsaBrWFyUdGh787IPA85ZJem+0TQKoTzuf9i+S9BtJ70oazB5+QtJ9km7U0GH/EUnfzz4cTK2LPT9QskIP+4tC+IHy8X1+AEmEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoHIv4FmwjyT9z7D7X88ea6Km9tbUviR661SRvf11u0+s9Pv8X9q4Wb+799TWQEJTe2tqXxK9daqu3jjsB4Ii/EBQdYd/fc3bT2lqb03tS6K3TtXSW63v+QHUp+49P4Ca1BJ+M7vTzA6b2ftm9ngdPbRiZkfM7N1s5uFapxjLpkEbMLP3hj12hZn92sz+kP0ccZq0mnprxMzNiZmla33tmjbjdeWH/WZ2iaTfS7pd0jFJeyXd5+6/q7SRFszsiKQed699TNjMbpb0F0k/Oz8bkpn9i6RT7v6j7A/n5e7+Tw3p7SmNcubmknprNbP0g6rxtStyxusi1LHnv0nS++7+R3c/K+nnkpbU0EfjuftuSacueniJpA3Z7Q0a+s9TuRa9NYK7n3D3fdntM5LOzyxd62uX6KsWdYT/aklHh90/pmZN+e2SdprZO2bWW3czI5h2fmak7OeVNfdzsdyZm6t00czSjXntOpnxumh1hH+k2USaNOSw0N3/VtI/SFqZHd6iPeskzdHQNG4nJP24zmaymaVflvQDd/9znb0MN0JftbxudYT/mKSZw+7PkHS8hj5G5O7Hs58Dkl7V0NuUJjl5fpLU7OdAzf38P3c/6e7n3H1Q0k9U42uXzSz9sqSN7v5K9nDtr91IfdX1utUR/r2S5prZN8xsoqTvSNpWQx9fYmaTsg9iZGaTJH1LzZt9eJukFdntFZK21tjLBZoyc3OrmaVV82vXtBmvaznJJxvKeFbSJZL63P2HlTcxAjO7RkN7e2noG4+b6uzNzDZLukVD3/o6KWmNpH+X9AtJsyT9SdK33b3yD95a9HaLRjlzc0m9tZpZ+m3V+NoVOeN1If1whh8QE2f4AUERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8I6v8AjJpR3rQ7fiYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i,(images,labels) in  enumerate(train_loader):\n",
    "    plt.imshow(images[0,0,:,:],cmap='gray');\n",
    "    if i > 1:\n",
    "        print(images.shape)\n",
    "        print(labels.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后来定义模型，和之前的一样，我们通过定义一个NeuralNet的类来定义模型内部的处理逻辑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再定义损失函数和优化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后来通过一个循环来训练，循环中包括了数据的读入，模型的前面预测，损失函数的计算，以及关键的参数的优化部分，同时将损失函数打印出来，以便观察。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.4890\n",
      "Epoch [1/5], Step [200/600], Loss: 0.3251\n",
      "Epoch [1/5], Step [300/600], Loss: 0.2102\n",
      "Epoch [1/5], Step [400/600], Loss: 0.3010\n",
      "Epoch [1/5], Step [500/600], Loss: 0.2985\n",
      "Epoch [1/5], Step [600/600], Loss: 0.1372\n",
      "Epoch [2/5], Step [100/600], Loss: 0.1911\n",
      "Epoch [2/5], Step [200/600], Loss: 0.2585\n",
      "Epoch [2/5], Step [300/600], Loss: 0.2275\n",
      "Epoch [2/5], Step [400/600], Loss: 0.1556\n",
      "Epoch [2/5], Step [500/600], Loss: 0.1711\n",
      "Epoch [2/5], Step [600/600], Loss: 0.2430\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0881\n",
      "Epoch [3/5], Step [200/600], Loss: 0.1549\n",
      "Epoch [3/5], Step [300/600], Loss: 0.1304\n",
      "Epoch [3/5], Step [400/600], Loss: 0.1202\n",
      "Epoch [3/5], Step [500/600], Loss: 0.1735\n",
      "Epoch [3/5], Step [600/600], Loss: 0.1484\n",
      "Epoch [4/5], Step [100/600], Loss: 0.1397\n",
      "Epoch [4/5], Step [200/600], Loss: 0.1233\n",
      "Epoch [4/5], Step [300/600], Loss: 0.1302\n",
      "Epoch [4/5], Step [400/600], Loss: 0.1807\n",
      "Epoch [4/5], Step [500/600], Loss: 0.1114\n",
      "Epoch [4/5], Step [600/600], Loss: 0.1248\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0450\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0387\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0640\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0906\n",
      "Epoch [5/5], Step [500/600], Loss: 0.1183\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0503\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        images = images.reshape(-1, 28*28)      \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在前面课程提到过，通过一个train数据集训练出来的模型，要通过test数据集来评估模型的效果，所以下面的部分是读入test的数据，计算准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97.18 %\n"
     ]
    }
   ],
   "source": [
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  用全连接神经网络判断服饰类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的例子准确率比较高，说明通过图片来判断数字是比较容易的。下面来尝试另一种更为困难的任务，也就是近年来更有趣的一类数据集fasion_mnist，它的每张图片还是28\\*28的大小，不过图片的内容是衣服或是包包这种服饰类的商品，我们来看一下这类数据建模怎么来做。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和之前的mnist一样，建立数据加载器，看其中一张图片看看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='../data/fasion_mnist', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='../data/fasion_mnist', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i,(images,labels) in  enumerate(train_loader):\n",
    "    plt.imshow(images[0,0,:,:],cmap='gray');\n",
    "    if i > 1:\n",
    "        print(images.shape)\n",
    "        print(labels.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义的模型和之前一样，只不过重新实例化了另一个对象model_fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fashion = NeuralNet(input_size, hidden_size, num_classes)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_fashion.parameters(), lr=learning_rate)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仍然是和前面例子类似的，用循环来训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 13.1476\n",
      "Epoch [1/5], Step [200/600], Loss: 13.7066\n",
      "Epoch [1/5], Step [300/600], Loss: 14.1717\n",
      "Epoch [1/5], Step [400/600], Loss: 13.8150\n",
      "Epoch [1/5], Step [500/600], Loss: 13.7365\n",
      "Epoch [1/5], Step [600/600], Loss: 12.2478\n",
      "Epoch [2/5], Step [100/600], Loss: 11.5349\n",
      "Epoch [2/5], Step [200/600], Loss: 15.2318\n",
      "Epoch [2/5], Step [300/600], Loss: 14.3791\n",
      "Epoch [2/5], Step [400/600], Loss: 13.6485\n",
      "Epoch [2/5], Step [500/600], Loss: 11.0129\n",
      "Epoch [2/5], Step [600/600], Loss: 13.0348\n",
      "Epoch [3/5], Step [100/600], Loss: 15.1056\n",
      "Epoch [3/5], Step [200/600], Loss: 12.5000\n",
      "Epoch [3/5], Step [300/600], Loss: 11.1531\n",
      "Epoch [3/5], Step [400/600], Loss: 13.3600\n",
      "Epoch [3/5], Step [500/600], Loss: 12.9815\n",
      "Epoch [3/5], Step [600/600], Loss: 11.3262\n",
      "Epoch [4/5], Step [100/600], Loss: 13.1235\n",
      "Epoch [4/5], Step [200/600], Loss: 11.5473\n",
      "Epoch [4/5], Step [300/600], Loss: 12.0653\n",
      "Epoch [4/5], Step [400/600], Loss: 13.2928\n",
      "Epoch [4/5], Step [500/600], Loss: 13.9749\n",
      "Epoch [4/5], Step [600/600], Loss: 11.9461\n",
      "Epoch [5/5], Step [100/600], Loss: 11.1002\n",
      "Epoch [5/5], Step [200/600], Loss: 12.3595\n",
      "Epoch [5/5], Step [300/600], Loss: 13.1775\n",
      "Epoch [5/5], Step [400/600], Loss: 11.8382\n",
      "Epoch [5/5], Step [500/600], Loss: 12.0068\n",
      "Epoch [5/5], Step [600/600], Loss: 12.3299\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        images = images.reshape(-1, 28*28)      \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 11.92 %\n"
     ]
    }
   ],
   "source": [
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，在这个数据集中，神经网络的方法就很差了，准确率只有11%多一点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 用卷积神经网络判断服饰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看来普通的神经网络无法很好的判断复杂的图片分类，下面我们使用更先进的一种神经网络，名字叫作卷积神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "num_epochs = 5\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型的计算逻辑，这里面的forward函数中，首先是一个卷积层，再经过relu激活，再过MaxPool池化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cnn_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            #nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(14*14*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.cnn_layer(x)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fashion_cnn = ConvNet(num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_fashion_cnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.5589\n",
      "Epoch [1/5], Step [200/600], Loss: 0.3763\n",
      "Epoch [1/5], Step [300/600], Loss: 0.3162\n",
      "Epoch [1/5], Step [400/600], Loss: 0.4692\n",
      "Epoch [1/5], Step [500/600], Loss: 0.3033\n",
      "Epoch [1/5], Step [600/600], Loss: 0.5682\n",
      "Epoch [2/5], Step [100/600], Loss: 0.4172\n",
      "Epoch [2/5], Step [200/600], Loss: 0.2394\n",
      "Epoch [2/5], Step [300/600], Loss: 0.4025\n",
      "Epoch [2/5], Step [400/600], Loss: 0.3059\n",
      "Epoch [2/5], Step [500/600], Loss: 0.2511\n",
      "Epoch [2/5], Step [600/600], Loss: 0.2924\n",
      "Epoch [3/5], Step [100/600], Loss: 0.3527\n",
      "Epoch [3/5], Step [200/600], Loss: 0.1705\n",
      "Epoch [3/5], Step [300/600], Loss: 0.4035\n",
      "Epoch [3/5], Step [400/600], Loss: 0.3289\n",
      "Epoch [3/5], Step [500/600], Loss: 0.3138\n",
      "Epoch [3/5], Step [600/600], Loss: 0.1943\n",
      "Epoch [4/5], Step [100/600], Loss: 0.2992\n",
      "Epoch [4/5], Step [200/600], Loss: 0.1685\n",
      "Epoch [4/5], Step [300/600], Loss: 0.2352\n",
      "Epoch [4/5], Step [400/600], Loss: 0.3416\n",
      "Epoch [4/5], Step [500/600], Loss: 0.2717\n",
      "Epoch [4/5], Step [600/600], Loss: 0.2839\n",
      "Epoch [5/5], Step [100/600], Loss: 0.2686\n",
      "Epoch [5/5], Step [200/600], Loss: 0.2972\n",
      "Epoch [5/5], Step [300/600], Loss: 0.1245\n",
      "Epoch [5/5], Step [400/600], Loss: 0.1621\n",
      "Epoch [5/5], Step [500/600], Loss: 0.1828\n",
      "Epoch [5/5], Step [600/600], Loss: 0.2116\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model_fashion_cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 89.45 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model_fashion_cnn(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本课小结：\n",
    "- 本课学习了神经网络在图片领域的应用，在数字识别上，全连接神经网络效果不错，但是在服饰识别上却效果不佳。\n",
    "- 所以引入了卷积神经网络，它的优点是对图片上的模式识别能力更强，泛化能力更好。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
