{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本课提纲\n",
    "- 二维格子世界\n",
    "- 环境模块gym\n",
    "- 基于Q学习的二维格子世界"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上一课中，我们学习了基础的强化学习知识，并且在一维格子空间中实行了一个强化学习的训练过程，那么我们来把问题搞的复杂一点，我们来看一下二维的格子世界中，应该如何来进行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二维格子世界"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二维的格子世界和一维格子世界有几个区别：\n",
    "1. 在二维世界中，机器人不仅可以左右行走，而且还可以前后行走，所以行动的选择有四种。\n",
    "2. 二维世界的边上都有墙，机器人不能走到外面去。二维格子世界是一个4\\*4的大小，所以一共有16个格子可以去。\n",
    "3. 二维世界中有起始点永远是在左上角，其它15个格子中大部分是安全的，而少数是存在陷阱，而右下解的格子是出口，到达会有奖励。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 环境模块gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在python中有一个非常用用的模块，专门用来模拟各种环境，这个模块叫作gym，它是由openAI网站做的模块，我们看一下可以怎么用它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import collections\n",
    "from  gym.envs.toy_text.frozen_lake import  FrozenLakeEnv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先加载一个叫作frozenlake的环境，也就是冰冻之湖，加载了这个环境之后，我们可以通过render函数来观察这个环境的情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrozenLakeEnv(is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面这个简图可以看到，这时有16个字符，排成了4\\*4的形式，也就是16个格子，其中S表示了起始点，F表示是冰冻的格子，H表示了有洞的地方，G表示我们的目的地。当机器人开始这个游戏时，永远是从S点开始，当它走到H的地方时，游戏终止，没有回报，当它走到G的地方时，游戏终止，得到回报1。所以我们需要训练机器人，让它学习到一种策略，能绕开冰洞，到达目的地，以得到最大的回报。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们看一下env可以有怎么样的函数，重要的是step函数，它的输入是机器人的动作，输出的主要有三项内容。例如输入的动作是2，那么第一项输出是动作后的状态，也就是新的位置编号，第二项输出是回报，第三项输出是反馈游戏有没有终止。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.0, False, {'prob': 1.0})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面我们输入2的动作之后，再观察环境，会发现机器人已经走到了起始点右边的格子上了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器人的动作有四个选择，分别用数字表示，0表示往左走，1表示往下走，2表示往右走，3表示往上走"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果走到游戏终止，还可以通env.reset来重启游戏。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于Q学习的二维世界"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先定义一些参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.5\n",
    "ALPHA = 0.1\n",
    "TEST_EPISODES = 30\n",
    "EPSILON = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们把所有的函数都用一个类来归集在一起，其中初始的几个属性比较重要，state是保存当前的位置，values是一个字典，它保存了Q表。feedback_env函数是向环境输入动作，得到环境反馈的功能。有一半的机会随机动作，一半的机会去执行最优动作。最优动作是由best_value_and_action函数负责计算的，它会遍历在某个状态下所有四个动作，看在Q表中哪个动作的Q值最大，那么就选择这个动作。如果所有的Q值都是0，也随机选择一个动作。value_update是用于更新参数的函数，它会将下一状态的Q值算出来，结合即时回报作为修正的目标Q值。play_episode是用于判断某个策略（Q表）的效果是怎么样的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = FrozenLakeEnv(is_slippery=False)\n",
    "        self.state = self.env.reset()\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "    def feedback_env(self):\n",
    "        old_state = self.state\n",
    "        if np.random.uniform() > EPSILON:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            _,action = self.best_value_and_action(old_state)\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.state = self.env.reset() if is_done else new_state\n",
    "        return (old_state, action, reward, new_state)\n",
    "\n",
    "\n",
    "   \n",
    "    def best_value_and_action(self, state):\n",
    "        best_value, best_action = None, None\n",
    "        # 遍历所有的动作\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.values[(state, action)]\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        if  best_value == 0:  \n",
    "            best_action = self.env.action_space.sample()\n",
    "            best_value = self.values[(state, best_action)]\n",
    "        return best_value, best_action\n",
    "    \n",
    "\n",
    "    def value_update(self, s, a, r, next_s):\n",
    "        best_v, _ = self.best_value_and_action(next_s)\n",
    "        new_val = r + GAMMA * best_v\n",
    "        old_val = self.values[(s, a)]\n",
    "        self.values[(s, a)] = old_val * (1-ALPHA) + new_val * ALPHA\n",
    "\n",
    "    def play_episode(self, env):  \n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            _, action = self.best_value_and_action(state)\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "            #self.value_update(self.state, action, reward, new_state)\n",
    "            total_reward += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们来用一个循环来计算得到最优策略，目标是用最少的循环次数，来使一次游戏的回报尽可能的高，可以设想，如果是完美的策略，基本上每次游戏都可以达到较高的回报，越接近于1越好，所以我们给出一个阈值，如果平均回报大于0.8，我们就终止实验。整体实验是一个while循环，每次循环中，有两个大的步骤，步骤一是机器人探索格子世界并修改参数，步骤二是用一个内层循环来检查本次修正后的效果，每次循环做20次游戏，然后得到平均回报，存到reward中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 0.033\n",
      "Best reward updated 0.033 -> 0.100\n",
      "Best reward updated 0.100 -> 0.133\n",
      "Best reward updated 0.133 -> 0.167\n",
      "Best reward updated 0.167 -> 0.200\n",
      "Best reward updated 0.200 -> 0.233\n",
      "Best reward updated 0.233 -> 0.267\n",
      "Best reward updated 0.267 -> 0.300\n",
      "Best reward updated 0.300 -> 0.400\n",
      "Best reward updated 0.400 -> 0.433\n",
      "Best reward updated 0.433 -> 0.533\n",
      "Best reward updated 0.533 -> 0.667\n",
      "Best reward updated 0.667 -> 0.700\n",
      "Best reward updated 0.700 -> 0.733\n",
      "Best reward updated 0.733 -> 0.800\n",
      "Best reward updated 0.800 -> 0.867\n",
      "Solved in 1734 iterations!\n"
     ]
    }
   ],
   "source": [
    "test_env = FrozenLakeEnv(is_slippery=False)\n",
    "agent = Agent()\n",
    "\n",
    "iter_no = 0\n",
    "best_reward = 0.0\n",
    "while True:\n",
    "    iter_no += 1\n",
    "    s, a, r, next_s = agent.feedback_env()\n",
    "    agent.value_update(s, a, r, next_s)\n",
    "    reward = 0.0\n",
    "    for i in range(TEST_EPISODES):\n",
    "        reward += agent.play_episode(test_env)\n",
    "    reward /= TEST_EPISODES\n",
    "    if reward > best_reward:\n",
    "        print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
    "        best_reward = reward\n",
    "    if reward > 0.80:\n",
    "        print(\"Solved in %d iterations!\" % iter_no)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在下面打印出机器人学到的Q表，也就是游戏策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {(0, 0): 0.0,\n",
       "             (0, 1): 0.0,\n",
       "             (0, 2): 0.0,\n",
       "             (0, 3): 0.0,\n",
       "             (4, 0): 0.0,\n",
       "             (4, 1): 6.250000000000003e-07,\n",
       "             (4, 2): 0.0,\n",
       "             (4, 3): 0.0,\n",
       "             (1, 0): 0.0,\n",
       "             (1, 1): 0.0,\n",
       "             (1, 2): 0.0,\n",
       "             (1, 3): 0.0,\n",
       "             (2, 0): 0.0,\n",
       "             (2, 1): 0.0,\n",
       "             (2, 2): 0.0,\n",
       "             (2, 3): 0.0,\n",
       "             (6, 0): 0.0,\n",
       "             (6, 1): 5.355625000000002e-06,\n",
       "             (6, 2): 0.0,\n",
       "             (6, 3): 0.0,\n",
       "             (8, 0): 0.0,\n",
       "             (8, 1): 0.0,\n",
       "             (8, 2): 3.500000000000001e-05,\n",
       "             (8, 3): 0.0,\n",
       "             (9, 0): 0.0,\n",
       "             (9, 1): 0.0039442750000000006,\n",
       "             (9, 2): 2.8187500000000008e-06,\n",
       "             (9, 3): 0.0,\n",
       "             (3, 0): 0.0,\n",
       "             (3, 1): 0.0,\n",
       "             (3, 2): 0.0,\n",
       "             (3, 3): 0.0,\n",
       "             (13, 0): 0.0,\n",
       "             (13, 1): 0.0,\n",
       "             (13, 2): 0.05057150000000001,\n",
       "             (13, 3): 0.0,\n",
       "             (10, 0): 0.00021251375000000007,\n",
       "             (10, 1): 0.0,\n",
       "             (10, 2): 0.0,\n",
       "             (10, 3): 0.0,\n",
       "             (14, 0): 0.0,\n",
       "             (14, 1): 0.0,\n",
       "             (14, 2): 0.40951000000000004,\n",
       "             (14, 3): 0.0,\n",
       "             (12, 0): 0.0,\n",
       "             (12, 1): 0.0,\n",
       "             (12, 2): 0.0,\n",
       "             (12, 3): 0.0,\n",
       "             (5, 0): 0.0,\n",
       "             (5, 1): 0.0,\n",
       "             (5, 2): 0.0,\n",
       "             (5, 3): 0.0,\n",
       "             (7, 0): 0.0,\n",
       "             (7, 1): 0.0,\n",
       "             (7, 2): 0.0,\n",
       "             (7, 3): 0.0,\n",
       "             (15, 0): 0.0,\n",
       "             (15, 1): 0.0,\n",
       "             (15, 2): 0.0,\n",
       "             (15, 3): 0.0,\n",
       "             (11, 0): 0.0,\n",
       "             (11, 1): 0.0,\n",
       "             (11, 2): 0.0,\n",
       "             (11, 3): 0.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
