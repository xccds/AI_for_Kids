{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本课提纲\n",
    "- 强化学习基本概念\n",
    "- Q学习的思想\n",
    "- 在一维空间中寻宝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 强化学习的基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在机器学习入门时提到过强化学习的一个例子。如果我们刚喂养了一只小狗，希望能训练它，因为它本身不懂人的语言，所以没法上课教育它，但是可以对它的行动进行矫正。如果它做出了对的行为，我们就进行一次奖励，如果它做出了错的行为，我们就进行一次惩罚。很快它就能明白应该去做哪些行动。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们把这个例子更加具体化一些，比如说这只小狗在一个迷宫中行走，迷宫中有的地方是墙壁，有的地方是陷阱，有的地方放着食物。小狗并不清楚整个迷宫的情况，每次它可以选择往四个方向中的任一个方向走一步，小狗每走完一步，并不会立刻得到反馈信息，要等到它找到食物或掉入陷阱后，才会有反馈。那么问题是它应该如何根据这种反馈，去自我学习使它吃到食物，而且避免落入那些陷阱呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种情况下并不适合用之前学到的有监督学习的情况，因为有监督学习需要在每一步之后都要对应一个反馈，但是小狗并不能马上得到反馈，它只能在走到特定的地方才能得到反馈。这种延迟的反馈在生活中经常会遇到，比方说，我们今天努力的读书学习，这个动作今天可能并不会马上有正面的反馈，但是期终考试或是未来的什么时候，今天的努力会得到正面的反馈。所以强化学习的应用场景是在没有丰富的即时的标签，但是又可以获得少量延时反馈的情况下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直觉上应该怎么解决这个问题呢？其实就是依靠试错和记忆。小狗一开始应该是随机的探索这个未知的迷宫世界，当它走到有食物的地方时候，它会明白之前的一系列努力是有效的，当它走到有陷阱的地方时候，会明白之前那些导致它走到陷阱的那些步子是有问题，需要回避的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习本质上就是试错和记忆，下面我们将直觉上的这些东西用一些正式的概念进行表达。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 主体（agent）:就是说谁来负责做出行动，谁来承受后果\n",
    "- 环境（envirnment）:对主体的行动作出反馈\n",
    "- 动作（action）:主体可以选择的行动有哪些\n",
    "- 状态（state）:主体的一系列状况。\n",
    "- 回报（reward）:主体在环境中进行了某些动作之后得到的回报"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们通过下面这个图来具体解释一下强化学习的几个概念。\n",
    "- 主体（agent）:一般是进行试错、学习的主体，本例中就是小狗，也可以叫做机器人。\n",
    "- 环境（envirnment）:本例中环境就是迷宫。\n",
    "- 动作（action）:本例中动作就是上下左右这四个动作，以决定行进的方向。\n",
    "- 状态（state）:本例中就是小狗在环境中的位置坐标。\n",
    "- 回报（reward）:本例中就是得到食物得到正的回报，或是落入陷阱得到负的回报。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总而言之，强化学习就是要训练小狗，通过自己走迷宫，和环境进行交互，再根据一系列动作后的回报，来找到走迷宫的策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Q学习的思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练小狗的目的是能够让它在迷宫世界中得到最大的正回报，也就是让它找到在这个迷宫中的最优行动策略，再具体一些，就是它要学会，给定某个状态时它会找到最优的行走方向，也就是走迷宫的策略。为了量化的计算，需要对每个状态下的每个动作来计算并保存它对于最终回报的贡献。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关键的问题是，如何把每一步的行动，和最终的回报联系起来。如果是一个有限的迷宫世界里，我们有一个穷举的思想来保存试错的结果，假设状态一共有10种，动作一共有4种，那么我们交叉一下就可以得到40个可能的组合，每个组合表示在某种状态下主体执行了某个动作。那么这个组合对于最终回报的贡献，我们用一个Q值来表示，那么所有的状态动作组合对的Q值，可以存在一个表里面，这就叫作Q表。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q表的用处在于，对于某一个状态，如果知道了所有动作的Q值，可以直接看哪种动作的Q值大，主体就直接选择这个动作去执行。但是要注意的是，强化学习很强调探索的重要性，试想一下，如果我们一开始在自己家周围试吃过两家餐厅，知道其中一家更好，那么我们会一直去这家吃饭吗？还是说也会尝试最远的地方的餐厅？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题随之而来了，如何去得到这个Q表，或者说如何计算到每个状态动作组合对的Q值？我们可以这样来设计，可以用0作为Q值的初始值，再看某个状态动作对是否会直接导致正回报，例如在迷宫出口边上的状态，下一步就可以拿到回报，所以这个状态动作对是一定可以算出Q值的，再看这个格子边上的格子，它的Q值也应该是正的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以Q值的计算是先初始化一个预计的Q值，然后根据探索试错的情况，得到真实的Q值，真实的Q值有两部分构成，一部分是这个行为得到的环境即时反馈回报，另一部分是这个行为导致的下一步状态带来的回报。从直觉上理解有这样一个例子，人们在高中毕业时有两个选择，一个是外出工作，一个是继续学习争取上大学，如果我们选择工作这个动作，它的即时回报是可以马上获得收入，但未来的成长性会低。上大学本身带来的即时回报比较低，但是未来的成长性会高，在大学毕业后带来的工资回报会较高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 在一维空间中寻宝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了简化问题，我们假设小狗生活的迷宫是一个一维的世界，也就是在一条线上面，线的右边是出口，它可以从出口处得到食物，线条长度是6。那么如何训练它的行动呢。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载必要的模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "np.random.seed(2)  # reproducible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置一些参数，这里面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STATES = 6   # 一维世界的长度\n",
    "ACTIONS = [0,1] # 有哪些动作可以选择，0表示向左走，1表示向右走\n",
    "N_ACTIOINS = 2   # 可选动作的个数\n",
    "EPSILON = 0.9   # greedy police\n",
    "ALPHA = 0.1     # learning rate\n",
    "GAMMA = 0.9    # discount factor\n",
    "MAX_EPISODES = 13   # maximum episodes\n",
    "FRESH_TIME = 0.3    # fresh time for one move\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们定义一个函数，来模拟环境的反馈，环境的输入是主体的当前状态和动作，环境的输出是主体的下一步状态和回报。这个函数中判断逻辑很简单，如何向右走，那么主体的位置坐标向右一格，如果到达了出口处，则状态变成“terminal”，同时回报获得+1。如果向左走，那么主体的位置坐标向左一格，如果碰到最左边的墙，则状态不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env_feedback(S, A):\n",
    "    # This is how agent will interact with the environment\n",
    "    if A == 1:    # move right\n",
    "        if S == N_STATES - 2:   # terminate\n",
    "            S_ = 'terminal'\n",
    "            R = 1\n",
    "        else:\n",
    "            S_ = S + 1\n",
    "            R = 0\n",
    "    else:   # move left\n",
    "        R = 0\n",
    "        if S == 0:\n",
    "            S_ = S  # reach the wall\n",
    "        else:\n",
    "            S_ = S - 1\n",
    "    return S_, R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一下，假设现处于编号为0的格子上，往右一步，结果是怎么样的，可以看到状态会转到编号为1的格子上，回报为0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_env_feedback(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后是建立一个函数来建立Q表，这个Q表是为了状态和动作组合对的Q值。初始化这个Q表的时候，里面的初始值全为0。可以看到这个Q表的大小是6\\*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立Q表，全为0填充\n",
    "def build_q_table(n_states, n_actions):\n",
    "    table = np.zeros((n_states, n_actions))\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立一个动作选择的函数，输入是当前状态和Q表，从Q表中找到状态下哪种动作的Q值最大，但是考虑到继续探索的动机，我们会保留一部分随机动作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, q_table):\n",
    "    # This is how to choose an action\n",
    "    state_actions = q_table[state, :]\n",
    "    if (np.random.uniform() > EPSILON) or ((state_actions == 0).all()):  \n",
    "        action_name = np.random.choice(ACTIONS)\n",
    "    else:   # act greedy\n",
    "        action_name = ACTIONS[state_actions.argmax()]   \n",
    "    return action_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了清楚的观察到主体在训练中的状态，我们建立一个函数，打印展示状态的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_env(S, episode, step_counter):\n",
    "    # This is how environment be updated\n",
    "    env_list = ['-']*(N_STATES-1) + ['T']   # '---------T' our environment\n",
    "    if S == 'terminal':\n",
    "        interaction = 'Episode %s: total_steps = %s' % (episode+1, step_counter)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(2)\n",
    "        print('\\r                                ', end='')\n",
    "    else:\n",
    "        env_list[S] = 'o'\n",
    "        interaction = ''.join(env_list)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(FRESH_TIME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最重要的训练函数，首先建立Q表，然后用双重循环来进行训练，内层循环是完成一个行为序列，一直试错直到抵达最右边的出口，外层循环中将这个试错过程重复多遍。重点是在内层循环中，一开始通过choose_action选择一个动作，然后通过get_env_feedback得到环境的反馈。q_target是指环境反馈后对Q值的重新估计，这里分两种情况，如果到达出口，重新估计的Q值就是回报R，如果没有到达出口，重新估计的Q值是回报R加上下一步状态的最大Q值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RL_Qlearning():\n",
    "    # main part of RL loop\n",
    "    q_table = build_q_table(N_STATES, N_ACTIOINS)\n",
    "    for episode in range(MAX_EPISODES):\n",
    "        step_counter = 0\n",
    "        S = 0\n",
    "        is_terminated = False\n",
    "        update_env(S, episode, step_counter)\n",
    "        while not is_terminated:\n",
    "            #print(q_table)\n",
    "            A = choose_action(S, q_table)\n",
    "            S_, R = get_env_feedback(S, A)  # take action & get next state and reward\n",
    "            q_predict = q_table[S, A]\n",
    "            if S_ != 'terminal':\n",
    "                q_target = R + GAMMA * q_table[S_,:].max()   # next state is not terminal\n",
    "            else:\n",
    "                q_target = R     # next state is terminal\n",
    "                is_terminated = True    # terminate this episode\n",
    "\n",
    "            q_table[S, A] += ALPHA * (q_target - q_predict)  # update\n",
    "            S = S_  # move to next state\n",
    "\n",
    "            update_env(S, episode, step_counter+1)\n",
    "            step_counter += 1\n",
    "    return q_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                \n",
      "Q-table:\n",
      "\n",
      "[[0.00000000e+00 4.31984803e-03]\n",
      " [0.00000000e+00 2.50048402e-02]\n",
      " [3.01806000e-05 1.11241084e-01]\n",
      " [0.00000000e+00 3.68750042e-01]\n",
      " [2.76208176e-02 7.45813417e-01]\n",
      " [0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "q_table = RL_Qlearning()\n",
    "print('\\r\\nQ-table:\\n')\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，第一次的学习训练比较困难，花费时间比较长，因为一开始的时候，主体并不知道怎么行动，更多的是碰运气式的游走。慢慢的，随着主体的阅历增长，对Q表的修正越来越好，最后的动作会非常的迅速准确。我们来观察一下最后得到的Q表，会发现第二列的值，都会大于第一列，也就是在每一个格子下，主体都应该向右走，这也是学到的最优策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本课小结：\n",
    "- 本课学习了强化学习的基本概念，它和传统的有监督学习不同，强化学习是基于交互行动进行学习。\n",
    "- 强化学习中最经典的是Q学习，也就是计算每个状态-行为对下的Q值，Q值是指这个状态行为对整体回报的贡献。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
